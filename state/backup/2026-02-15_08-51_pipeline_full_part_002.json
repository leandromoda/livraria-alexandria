e)\n\n    if not rows:\n        log(\"Nada pendente para dedup.\")\n        return\n\n    processed = 0\n    removed = 0\n\n    for r in rows:\n\n        book = {\n            \"id\": r[0],\n            \"titulo\": r[1],\n            \"slug\": r[2],\n            \"isbn\": r[3],\n        }\n\n        duplicates = find_duplicates(book)\n\n        for dup in duplicates:\n            merge_books(book[\"id\"], dup)\n            removed += 1\n\n        mark_processed(book[\"id\"])\n\n        processed += 1\n        log(f\"DEDUP OK → {book['titulo']}\")\n\n    log(\n        f\"DEDUP CONCLUÍDO → processados {processed} | removidos {removed}\"\n    )\n",
    "scripts\\steps\\export_state_transcript.py": "import os\nimport json\nimport sqlite3\nfrom datetime import datetime\nfrom pathlib import Path\n\n\n# =========================\n# ROOT\n# =========================\n\nPROJECT_ROOT = Path(__file__).resolve().parents[2]\nSTATE_DIR = PROJECT_ROOT / \"state\"\n\nSTATE_DIR.mkdir(exist_ok=True)\n\n\n# =========================\n# CONFIG\n# =========================\n\nMAX_CHARS = 25000\n\nSITE_EXTENSIONS = [\".tsx\", \".ts\", \".css\", \".json\"]\nPIPELINE_EXTENSIONS = [\".py\", \".json\", \".db\"]\n\nSITE_FOLDERS = [\"app\", \"lib\", \"public\"]\nPIPELINE_FOLDERS = [\"scripts\", \"state\"]\n\nEXCLUDE_DIRS = {\n    \"node_modules\",\n    \".next\",\n    \"venv\",\n    \"__pycache__\"\n}\n\nSQLITE_DB_PATH = PROJECT_ROOT / \"scripts\" / \"data\" / \"books.db\"\n\n\n# =========================\n# UTILS\n# =========================\n\ndef now():\n    return datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n\n\n# =========================\n# SPLIT\n# =========================\n\ndef split_text(text):\n\n    parts = []\n    start = 0\n\n    while start < len(text):\n        parts.append(text[start:start + MAX_CHARS])\n        start += MAX_CHARS\n\n    return parts\n\n\ndef write_parts(name, data):\n\n    text = json.dumps(data, indent=2, ensure_ascii=False)\n\n    parts = split_text(text)\n\n    paths = []\n\n    for i, part in enumerate(parts, 1):\n\n        file = STATE_DIR / f\"{name}_part_{i:03}.json\"\n\n        with open(file, \"w\", encoding=\"utf-8\") as f:\n            f.write(part)\n\n        paths.append(file)\n\n    return paths\n\n\n# =========================\n# TREE (SITE REDUCED)\n# =========================\n\ndef build_site_tree():\n\n    lines = []\n\n    for folder in SITE_FOLDERS:\n\n        base = PROJECT_ROOT / folder\n\n        if not base.exists():\n            continue\n\n        for root, dirs, files in os.walk(base):\n\n            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]\n\n            rel = Path(root).relative_to(PROJECT_ROOT)\n\n            lines.append(str(rel))\n\n    return \"\\n\".join(lines)\n\n\n# =========================\n# FILE COLLECT\n# =========================\n\ndef collect_files(folders, extensions=None):\n\n    collected = {}\n\n    for folder in folders:\n\n        base = PROJECT_ROOT / folder\n\n        if not base.exists():\n            continue\n\n        for root, dirs, files in os.walk(base):\n\n            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]\n\n            for file in files:\n\n                if extensions and not any(\n                    file.endswith(ext) for ext in extensions\n                ):\n                    continue\n\n                path = Path(root) / file\n                rel = str(path.relative_to(PROJECT_ROOT))\n\n                if file.endswith(\".db\"):\n                    collected[rel] = dump_sqlite(path)\n                else:\n                    collected[rel] = read_file(path)\n\n    return collected\n\n\n# =========================\n# FILE READ\n# =========================\n\ndef read_file(path):\n\n    try:\n        return path.read_text(encoding=\"utf-8\")\n    except:\n        return \"[binary or unreadable]\"\n\n\n# =========================\n# SQLITE DUMP\n# =========================\n\ndef dump_sqlite(path):\n\n    try:\n\n        conn = sqlite3.connect(path)\n        cursor = conn.cursor()\n\n        cursor.execute(\n            \"SELECT name FROM sqlite_master WHERE type='table';\"\n        )\n\n        tables = [t[0] for t in cursor.fetchall()]\n\n        dump = {}\n\n        for table in tables:\n\n            cursor.execute(f\"PRAGMA table_info({table});\")\n            cols = cursor.fetchall()\n\n            dump[table] = [\n                {\n                    \"name\": c[1],\n                    \"type\": c[2]\n                }\n                for c in cols\n            ]\n\n        conn.close()\n\n        return dump\n\n    except:\n        return {\"error\": \"failed to read sqlite\"}\n\n\n# =========================\n# DB ABSTRACT\n# =========================\n\ndef load_project_state():\n\n    path = PROJECT_ROOT / \"project_state.json\"\n\n    if path.exists():\n        return path.read_text(encoding=\"utf-8\")\n\n    return \"project_state.json não encontrado.\"\n\n\ndef load_db_schema():\n\n    path = PROJECT_ROOT / \"database_schema.json\"\n\n    if path.exists():\n        return path.read_text(encoding=\"utf-8\")\n\n    return \"database_schema.json não encontrado.\"\n\n\n# =========================\n# MODES\n# =========================\n\ndef export_site():\n\n    name = f\"{now()}_site_bootstrap\"\n\n    data = {\n\n        \"tree_site\": build_site_tree(),\n\n        \"project_state\": load_project_state(),\n\n        \"database_schema_abstract\": load_db_schema(),\n\n        \"files\": collect_files(\n            SITE_FOLDERS,\n            SITE_EXTENSIONS\n        )\n    }\n\n    return write_parts(name, data)\n\n\ndef export_pipeline():\n\n    name = f\"{now()}_pipeline_full\"\n\n    data = {\n\n        \"sqlite_dump\": dump_sqlite(SQLITE_DB_PATH),\n\n        \"files\": collect_files(\n            PIPELINE_FOLDERS,\n            PIPELINE_EXTENSIONS\n        )\n    }\n\n    return write_parts(name, data)\n\n\ndef export_full():\n\n    name = f\"{now()}_full_snapshot\"\n\n    data = {\n\n        \"tree_site\": build_site_tree(),\n\n        \"sqlite_dump\": dump_sqlite(SQLITE_DB_PATH),\n\n        \"files\": collect_files(\n            [\".\"],\n            None\n        )\n    }\n\n    return write_parts(name, data)\n\n\n# =========================\n# ENTRY\n# =========================\n\ndef export_state_transcript(mode=\"site\"):\n\n    if mode == \"site\":\n        paths = export_site()\n\n    elif mode == \"pipeline\":\n        paths = export_pipeline()\n\n    elif mode == \"full\":\n        paths = export_full()\n\n    else:\n        print(\"Modo inválido.\")\n        return\n\n    print(\"\\nTranscript gerado:\\n\")\n\n    for p in paths:\n        print(p)\n\n    print(\"\\nTotal partes:\", len(paths), \"\\n\")\n\n\n# =========================\n\nif __name__ == \"__main__\":\n    export_state_transcript(\"site\")\n",
    "scripts\\steps\\prospect.py": "# ============================================\n# LIVRARIA ALEXANDRIA — PROSPECT\n# Compatível com books.db (schema staging)\n# ID Strategy: UUID v4 + fallback hash título\n# ============================================\n\nimport os\nimport time\nimport uuid\nimport hashlib\nimport sqlite3\nimport requests\n\nfrom datetime import datetime\nfrom steps._clusters import CLUSTERS\n\n\n# ============================================\n# CONFIG\n# ============================================\n\nDB_PATH = os.path.join(\n    os.path.dirname(__file__),\n    \"..\",\n    \"data\",\n    \"books.db\"\n)\n\nOPENLIBRARY_URL = \"https://openlibrary.org/search.json\"\nGOOGLE_BOOKS_URL = \"https://www.googleapis.com/books/v1/volumes\"\n\nHEARTBEAT_INTERVAL = 30\n\n\n# ============================================\n# HEARTBEAT\n# ============================================\n\n_last_event = time.time()\n\n\ndef ts():\n    return datetime.now().strftime(\"%H:%M:%S\")\n\n\ndef beat(msg=\"Script ativo…\"):\n    global _last_event\n\n    now_ts = time.time()\n\n    if now_ts - _last_event >= HEARTBEAT_INTERVAL:\n        print(\n            f\"[{ts()}] {msg} último evento há {int(now_ts - _last_event)}s\"\n        )\n\n    _last_event = now_ts\n\n\n# ============================================\n# DB\n# ============================================\n\ndef get_conn():\n    return sqlite3.connect(DB_PATH)\n\n\ndef ensure_schema():\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    cur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS livros (\n        id TEXT PRIMARY KEY,\n        titulo TEXT NOT NULL,\n        slug TEXT UNIQUE,\n        autor TEXT,\n        descricao TEXT,\n        isbn TEXT,\n        ano_publicacao INTEGER,\n        imagem_url TEXT,\n        idioma TEXT,\n        cluster TEXT,\n        fonte TEXT,\n        status_slug INTEGER DEFAULT 0,\n        status_dedup INTEGER DEFAULT 0,\n        status_synopsis INTEGER DEFAULT 0,\n        status_review INTEGER DEFAULT 0,\n        status_cover INTEGER DEFAULT 0,\n        status_publish INTEGER DEFAULT 0,\n        created_at DATETIME,\n        updated_at DATETIME\n    )\n    \"\"\")\n\n    conn.commit()\n    conn.close()\n\n\n# ============================================\n# ID STRATEGY\n# ============================================\n\ndef generate_id(titulo, isbn):\n\n    if isbn:\n        base = isbn\n    else:\n        base = titulo\n\n    hash_id = hashlib.sha1(\n        base.encode(\"utf-8\")\n    ).hexdigest()\n\n    return str(uuid.uuid4())[:8] + hash_id[:16]\n\n\n# ============================================\n# INSERT\n# ============================================\n\ndef exists(titulo):\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    cur.execute(\n        \"SELECT 1 FROM livros WHERE titulo = ? LIMIT 1\",\n        (titulo,)\n    )\n\n    res = cur.fetchone()\n    conn.close()\n\n    return res is not None\n\n\ndef insert_book(data, cluster):\n\n    if exists(data[\"titulo\"]):\n        print(f\"[{ts()}] SKIP duplicado → {data['titulo']}\")\n        return False\n\n    livro_id = generate_id(\n        data[\"titulo\"],\n        data[\"isbn\"]\n    )\n\n    now = datetime.utcnow()\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    cur.execute(\"\"\"\n        INSERT INTO livros (\n            id,\n            titulo,\n            autor,\n            isbn,\n            ano_publicacao,\n            idioma,\n            cluster,\n            fonte,\n            created_at,\n            updated_at\n        )\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n    \"\"\", (\n        livro_id,\n        data[\"titulo\"],\n        data[\"autor\"],\n        data[\"isbn\"],\n        data[\"ano\"],\n        data[\"idioma\"],\n        cluster,\n        data[\"fonte\"],\n        now,\n        now\n    ))\n\n    conn.commit()\n    conn.close()\n\n    return True\n\n\n# ============================================\n# FETCHERS\n# ============================================\n\ndef fetch_openlibrary(query, idioma, limit=20):\n\n    beat()\n\n    try:\n        res = requests.get(\n            OPENLIBRARY_URL,\n            params={\n                \"q\": query,\n                \"language\": idioma,\n                \"limit\": limit\n            },\n            timeout=20\n        )\n\n        docs = res.json().get(\"docs\", [])\n\n        books = []\n\n        for d in docs:\n\n            titulo = d.get(\"title\")\n            autores = \", \".join(d.get(\"author_name\", []))\n\n            isbn_list = d.get(\"isbn\", [])\n            isbn = isbn_list[0] if isbn_list else None\n\n            ano = d.get(\"first_publish_year\")\n\n            if not titulo:\n                continue\n\n            books.append({\n                \"titulo\": titulo,\n                \"autor\": autores,\n                \"isbn\": isbn,\n                \"ano\": ano,\n                \"idioma\": idioma,\n                \"fonte\": \"openlibrary\"\n            })\n\n        return books\n\n    except Exception as e:\n        print(f\"[{ts()}] ERRO OL → {e}\")\n        return []\n\n\ndef fetch_google(query, idioma, limit=20):\n\n    beat()\n\n    try:\n        res = requests.get(\n            GOOGLE_BOOKS_URL,\n            params={\n                \"q\": query,\n                \"maxResults\": limit,\n                \"langRestrict\": idioma\n            },\n            timeout=20\n        )\n\n        items = res.json().get(\"items\", [])\n\n        books = []\n\n        for item in items:\n\n            info = item.get(\"volumeInfo\", {})\n\n            titulo = info.get(\"title\")\n            autores = \", \".join(info.get(\"authors\", []))\n\n            industry = info.get(\"industryIdentifiers\", [])\n\n            isbn = None\n            for i in industry:\n                if \"ISBN\" in i[\"type\"]:\n                    isbn = i[\"identifier\"]\n                    break\n\n            ano = info.get(\"publishedDate\", \"\")[:4]\n\n            if not titulo:\n                continue\n\n            books.append({\n                \"titulo\": titulo,\n                \"autor\": autores,\n                \"isbn\": isbn,\n                \"ano\": ano,\n                \"idioma\": idioma,\n                \"fonte\": \"google\"\n            })\n\n        return books\n\n    except Exception as e:\n        print(f\"[{ts()}] ERRO GOOGLE → {e}\")\n        return []\n\n\n# ============================================\n# RUN\n# ============================================\n\ndef run(idioma, pacote):\n\n    ensure_schema()\n\n    inseridos = 0\n\n    for cluster, queries in CLUSTERS.items():\n\n        print(f\"[{ts()}] CLUSTER → {cluster}\")\n\n        for query in queries:\n\n            if inseridos >= pacote:\n                print(f\"[{ts()}] Pacote atingido — STOP.\")\n                return\n\n            print(f\"[{ts()}] QUERY → {query}\")\n\n            ol_books = fetch_openlibrary(query, idioma)\n            g_books = fetch_google(query, idioma)\n\n            for book in ol_books + g_books:\n\n                if inseridos >= pacote:\n                    print(f\"[{ts()}] Pacote atingido — STOP.\")\n                    return\n\n                if insert_book(book, cluster):\n                    inseridos += 1\n                    print(\n                        f\"[{ts()}] INSERT {inseridos}/{pacote} → {book['titulo']}\"\n                    )\n\n    print(f\"[{ts()}] Fim da prospecção.\")\n\n\n# ============================================\n# ENTRYPOINT\n# ============================================\n\nif __name__ == \"__main__\":\n    run(\"pt\", 10)\n",
    "scripts\\steps\\publish.py": "import requests\nimport uuid\nfrom datetime import datetime\n\nfrom core.db import get_conn\nfrom core.logger import log\n\n# =========================\n# CONFIG\n# =========================\n\nSUPABASE_URL = \"https://ncnexkuiiuzwujqurtsa.supabase.co\"\nSUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im5jbmV4a3VpaXV6d3VqcXVydHNhIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTU0MTY2MCwiZXhwIjoyMDg1MTE3NjYwfQ.CacLDlVd0noDzcuVJnxjx3eMr7SjI_19rAsDZeQh6S8\"\n\nHEADERS = {\n    \"apikey\": SUPABASE_KEY,\n    \"Authorization\": f\"Bearer {SUPABASE_KEY}\",\n    \"Content-Type\": \"application/json\",\n    \"Prefer\": \"resolution=merge-duplicates\"\n}\n\nTABLE_URL = f\"{SUPABASE_URL}/rest/v1/livros\"\n\n\n# =========================\n# FETCH\n# =========================\n\ndef fetch_pending(limit):\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    cur.execute(\"\"\"\n        SELECT\n            titulo,\n            slug,\n            autor,\n            descricao,\n            isbn,\n            ano_publicacao,\n            imagem_url,\n            id\n        FROM livros\n        WHERE status_publish = 0\n        AND status_review = 1\n        LIMIT ?\n    \"\"\", (limit,))\n\n    rows = cur.fetchall()\n    conn.close()\n\n    return rows\n\n\n# =========================\n# PAYLOAD\n# =========================\n\ndef build_payload(row):\n\n    now = datetime.utcnow().isoformat()\n\n    return {\n        \"id\": str(uuid.uuid4()),   # ← UUID válido\n        \"titulo\": row[0],\n        \"slug\": row[1],\n        \"autor\": row[2],\n        \"descricao\": row[3],\n        \"isbn\": row[4],\n        \"ano_publicacao\": row[5],\n        \"imagem_url\": row[6],\n        \"created_at\": now,\n        \"updated_at\": now,\n    }\n\n\n# =========================\n# UPSERT\n# =========================\n\ndef upsert_book(payload):\n\n    res = requests.post(\n        TABLE_URL,\n        headers=HEADERS,\n        json=payload\n    )\n\n    if res.status_code not in [200, 201]:\n        print(\"STATUS:\", res.status_code)\n        print(\"BODY:\", res.text)\n\n    return res.status_code in [200, 201]\n\n\n# =========================\n# FLAG\n# =========================\n\ndef mark_published(local_id):\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    cur.execute(\"\"\"\n        UPDATE livros\n        SET\n            status_publish = 1,\n            updated_at = CURRENT_TIMESTAMP\n        WHERE id = ?\n    \"\"\", (local_id,))\n\n    conn.commit()\n    conn.close()\n\n\n# =========================\n# RUN\n# =========================\n\ndef run(pacote=10):\n\n    rows = fetch_pending(pacote)\n\n    if not rows:\n        log(\"Nada pendente para publicação.\")\n        return\n\n    inserted = 0\n    failed = 0\n\n    for row in rows:\n\n        payload = build_payload(row)\n\n        ok = upsert_book(payload)\n\n        if not ok:\n            failed += 1\n            log(f\"FALHA → {row[0]}\")\n            continue\n\n        mark_published(row[7])\n\n        inserted += 1\n        log(f\"PUBLICADO → {row[0]}\")\n\n    log(\n        f\"PUBLICAÇÃO CONCLUÍDA → {inserted} | falhas {failed}\"\n    )\n",
    "scripts\\steps\\review.py": "import requests\nimport re\nimport unicodedata\nimport time\n\nfrom core.db import get_conn\nfrom core.logger import log\n\n# =========================\n# CONFIG\n# =========================\n\nOLLAMA_URL = \"http://localhost:11434/api/generate\"\nMODEL = \"phi3:mini\"\n\nTIMEOUT = 300\nMAX_RETRIES = 3\n\n\n# =========================\n# PROMPT\n# =========================\n\ndef build_prompt(texto):\n\n    return f\"\"\"\nRevise a sinopse abaixo.\n\nObjetivos:\n\n- Corrigir gramática\n- Corrigir concordância\n- Melhorar fluidez\n- Remover repetições\n- Máx 80 palavras\n- Não adicionar conteúdo novo\n\nSinopse:\n\n{texto}\n\"\"\"\n\n\n# =========================\n# LLM CALL\n# =========================\n\ndef review_text(texto):\n\n    for attempt in range(MAX_RETRIES):\n\n        try:\n\n            res = requests.post(\n                OLLAMA_URL,\n                json={\n                    \"model\": MODEL,\n                    \"prompt\": build_prompt(texto),\n                    \"stream\": False,\n                    \"options\": {\n                        \"temperature\": 0.2,\n                        \"num_predict\": 180\n                    }\n                },\n                timeout=TIMEOUT\n            )\n\n            text = res.json()[\"response\"].strip()\n\n            if len(text) > 30:\n                return text\n\n        except Exception:\n            log(f\"Retry Review ({attempt+1})\")\n            time.sleep(2)\n\n    return None\n\n\n# =========================\n# SLUG\n# =========================\n\ndef base_slug(text):\n\n    text = unicodedata.normalize(\"NFKD\", text)\n    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    text = text.lower()\n\n    text = re.sub(r\"[^a-z0-9\\s-]\", \"\", text)\n    text = re.sub(r\"\\s+\", \"-\", text)\n\n    return text.strip(\"-\")\n\n\ndef slug_exists(slug, book_id):\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    cur.execute(\"\"\"\n        SELECT 1\n        FROM livros\n        WHERE slug = ?\n        AND id != ?\n        LIMIT 1\n    \"\"\", (slug, book_id))\n\n    exists = cur.fetchone() is not None\n    conn.close()\n\n    return exists\n\n\ndef revise_slug(titulo, book_id):\n\n    base = base_slug(titulo)\n    slug = base\n    counter = 2\n\n    while slug_exists(slug, book_id):\n        slug = f\"{base}-{counter}\"\n        counter += 1\n\n    return slug\n\n\n# =========================\n# FETCH PENDENTES\n# =========================\n\ndef fetch_pending(limit):\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    cur.execute(\"\"\"\n        SELECT id, titulo, descricao\n        FROM livros\n        WHERE status_synopsis = 1\n        AND status_review = 0\n        LIMIT ?\n    \"\"\", (limit,))\n\n    rows = cur.fetchall()\n    conn.close()\n\n    return rows\n\n\n# =========================\n# UPDATE\n# =========================\n\ndef update_review(book_id, texto, slug):\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    cur.execute(\"\"\"\n        UPDATE livros\n        SET\n            descricao = ?,\n            slug = ?,\n            status_review = 1,\n            updated_at = CURRENT_TIMESTAMP\n        WHERE id = ?\n    \"\"\", (texto, slug, book_id))\n\n    conn.commit()\n    conn.close()\n\n\n# =========================\n# RUN\n# =========================\n\ndef run(pacote=10):\n\n    rows = fetch_pending(pacote)\n\n    if not rows:\n        log(\"Nada pendente para revisão.\")\n        return\n\n    processed = 0\n    failed = 0\n\n    for book_id, titulo, descricao in rows:\n\n        log(f\"REVISANDO → {titulo}\")\n\n        texto_revisto = review_text(descricao)\n\n        if not texto_revisto:\n            failed += 1\n            log(f\"FALHA REVIEW → {titulo}\")\n            continue\n\n        slug_revisto = revise_slug(titulo, book_id)\n\n        update_review(\n            book_id,\n            texto_revisto,\n            slug_revisto\n        )\n\n        processed += 1\n\n        log(f\"REVISADO → {titulo}\")\n\n    log(\n        f\"REVISÃO CONCLUÍDA → {processed} | falhas {failed}\"\n    )\n",
    "scripts\\steps\\slugify.py": "import re\nimport unicodedata\nimport os\nimport sqlite3\n\nfrom datetime import datetime\n\n\n# =========================\n# DB PATH (ALINHADO PROSPECT)\n# =========================\n\nDB_PATH = os.path.join(\n    os.path.dirname(__file__),\n    \"..\",\n    \"data\",\n    \"books.db\"\n)\n\n\ndef get_conn():\n    return sqlite3.connect(DB_PATH)\n\n\n# =========================\n# LOGGER SIMPLES\n# =========================\n\ndef log(msg):\n    now = datetime.now().strftime(\"%H:%M:%S\")\n    print(f\"[{now}] {msg}\")\n\n\n# =========================\n# SLUG BASE\n# =========================\n\ndef base_slug(text):\n\n    text = unicodedata.normalize(\"NFKD\", text)\n    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    text = text.lower()\n\n    text = re.sub(r\"[^a-z0-9\\s-]\", \"\", text)\n    text = re.sub(r\"\\s+\", \"-\", text)\n\n    return text.strip(\"-\")\n\n\n# =========================\n# CHECK COLISÃO\n# =========================\n\ndef slug_exists(slug):\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    cur.execute(\n        \"SELECT 1 FROM livros WHERE slug = ? LIMIT 1\",\n        (slug,)\n    )\n\n    exists = cur.fetchone() is not None\n\n    conn.close()\n\n    return exists\n\n\ndef generate_unique_slug(titulo):\n\n    base = base_slug(titulo)\n\n    slug = base\n    counter = 2\n\n    while slug_exists(slug):\n        slug = f\"{base}-{counter}\"\n        counter += 1\n\n    return slug\n\n\n# =========================\n# FETCH PENDENTES\n# =========================\n\ndef fetch_pending(limit):\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    cur.execute(\"\"\"\n        SELECT id, titulo\n        FROM livros\n        WHERE status_slug = 0\n        LIMIT ?\n    \"\"\", (limit,))\n\n    rows = cur.fetchall()\n    conn.close()\n\n    return rows\n\n\n# =========================\n# UPDATE\n# =========================\n\ndef update_slug(book_id, slug):\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    now = datetime.utcnow()\n\n    cur.execute(\"\"\"\n        UPDATE livros\n        SET slug = ?,\n            status_slug = 1,\n            updated_at = ?\n        WHERE id = ?\n    \"\"\", (slug, now, book_id))\n\n    conn.commit()\n    conn.close()\n\n\n# =========================\n# RUN\n# =========================\n\ndef run(pacote=10):\n\n    rows = fetch_pending(pacote)\n\n    if not rows:\n        log(\"Nada pendente para slug.\")\n        return\n\n    processed = 0\n\n    for book_id, titulo in rows:\n\n        slug = generate_unique_slug(titulo)\n\n        update_slug(book_id, slug)\n\n        processed += 1\n\n        log(f\"SLUG → {titulo} → {slug}\")\n\n    log(f\"SLUG CONCLUÍDO → {processed}\")\n",
    "scripts\\steps\\synopsis.py": "import requests\nimport time\nimport os\nimport sqlite3\n\nfrom datetime import datetime\n\n\n# =========================\n# DB PATH\n# =========================\n\nDB_PATH = os.path.join(\n    os.path.dirname(__file__),\n    \"..\",\n    \"data\",\n    \"books.db\"\n)\n\n\ndef get_conn():\n    return sqlite3.connect(DB_PATH, 