nst.subheader(\\\"Base local\\\")\\n\\nst.dataframe(\\n    df[[\\n        \\\"titulo\\\",\\n        \\\"autor\\\",\\n        \\\"isbn\\\",\\n        \\\"slug\\\",\\n        \\\"sinopse\\\",\\n        \\\"revisado\\\",\\n        \\\"capa\\\",\\n        \\\"publicado\\\"\\n    ]],\\n    use_container_width=True\\n)\\n\\n\\n# =========================\\n# FILTROS\\n# =========================\\n\\nst.subheader(\\\"Filtrar pendências\\\")\\n\\nfiltro = st.selectbox(\\n    \\\"Etapa pendente\\\",\\n    [\\n        \\\"Sinopse\\\",\\n        \\\"Revisão\\\",\\n        \\\"Capa\\\",\\n        \\\"Publicação\\\"\\n    ]\\n)\\n\\nif filtro == \\\"Sinopse\\\":\\n    pend = df[df['sinopse'] == 0]\\n\\nelif filtro == \\\"Revisão\\\":\\n    pend = df[df['revisado'] == 0]\\n\\nelif filtro == \\\"Capa\\\":\\n    pend = df[df['capa'] == 0]\\n\\nelif filtro == \\\"Publicação\\\":\\n    pend = df[df['publicado'] == 0]\\n\\nst.write(f\\\"Pendentes: {len(pend)}\\\")\\n\\nst.dataframe(\\n    pend[[\\\"titulo\\\", \\\"autor\\\", \\\"slug\\\"]],\\n    use_container_width=True\\n)\\n\\n\\n# =========================\\n# AUTO REFRESH\\n# =========================\\n\\nst.caption(\\\"Atualize a página para refletir progresso em tempo real.\\\")\\n\",\n    \"scripts\\\\data\\\\books.db\": {\n      \"livros\": [\n        {\n          \"name\": \"id\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"titulo\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"slug\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"autor\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"descricao\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"isbn\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"ano_publicacao\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"imagem_url\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"idioma\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"cluster\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"fonte\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"status_slug\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"status_dedup\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"status_synopsis\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"status_review\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"status_cover\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"status_publish\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"created_at\",\n          \"type\": \"DATETIME\"\n        },\n        {\n          \"name\": \"updated_at\",\n          \"type\": \"DATETIME\"\n        }\n      ]\n    },\n    \"scripts\\\\scripts\\\\data\\\\books.db\": {\n      \"books\": [\n        {\n          \"name\": \"id\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"titulo\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"autor\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"isbn\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"ano_publicacao\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"descricao\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"descricao_revisada\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"slug\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"imagem_url\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"idioma\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"created_at\",\n          \"type\": \"TIMESTAMP\"\n        },\n        {\n          \"name\": \"prospectado\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"slugger\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"dedup\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"sinopse\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"revisado\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"capa\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"publicado\",\n          \"type\": \"INTEGER\"\n        }\n      ],\n      \"livros\": [\n        {\n          \"name\": \"id\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"titulo\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"slug\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"autor\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"descricao\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"isbn\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"ano_publicacao\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"imagem_url\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"idioma\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"cluster\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"fonte\",\n          \"type\": \"TEXT\"\n        },\n        {\n          \"name\": \"status_slug\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"status_dedup\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"status_synopsis\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"status_review\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"status_cover\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"status_publish\",\n          \"type\": \"INTEGER\"\n        },\n        {\n          \"name\": \"created_at\",\n          \"type\": \"DATETIME\"\n        },\n        {\n          \"name\": \"updated_at\",\n          \"type\": \"DATETIME\"\n        }\n      ]\n    },\n    \"scripts\\\\steps\\\\covers.py\": \"import requests\\nimport time\\n\\nfrom core.db import get_conn\\nfrom core.logger import log\\n\\n# =========================\\n# CONFIG\\n# =========================\\n\\nOPENLIBRARY_COVER = \\\"https://covers.openlibrary.org/b/isbn/{isbn}-L.jpg\\\"\\nGOOGLE_BOOKS_URL = \\\"https://www.googleapis.com/books/v1/volumes\\\"\\n\\nTIMEOUT = 60\\n\\n\\n# =========================\\n# OPENLIBRARY\\n# =========================\\n\\ndef fetch_openlibrary_cover(isbn):\\n\\n    if not isbn:\\n        return None\\n\\n    url = OPENLIBRARY_COVER.format(isbn=isbn)\\n\\n    try:\\n        res = requests.get(url, timeout=TIMEOUT)\\n\\n        if res.status_code == 200 and res.content:\\n            return url\\n\\n    except:\\n        pass\\n\\n    return None\\n\\n\\n# =========================\\n# GOOGLE\\n# =========================\\n\\ndef fetch_google_cover(titulo, autor):\\n\\n    query = f\\\"{titulo} {autor}\\\"\\n\\n    try:\\n\\n        res = requests.get(\\n            GOOGLE_BOOKS_URL,\\n            params={\\\"q\\\": query, \\\"maxResults\\\": 1},\\n            timeout=TIMEOUT\\n        )\\n\\n        items = res.json().get(\\\"items\\\")\\n\\n        if not items:\\n            return None\\n\\n        links = items[0][\\\"volumeInfo\\\"].get(\\n            \\\"imageLinks\\\", {}\\n        )\\n\\n        thumb = (\\n            links.get(\\\"thumbnail\\\")\\n            or links.get(\\\"smallThumbnail\\\")\\n        )\\n\\n        if thumb:\\n            return thumb.replace(\\n                \\\"http://\\\",\\n                \\\"https://\\\"\\n            )\\n\\n    except:\\n        pass\\n\\n    return None\\n\\n\\n# =========================\\n# FETCH PENDENTES\\n# =========================\\n\\ndef fetch_pending(limit):\\n\\n    conn = get_conn()\\n    cur = conn.cursor()\\n\\n    cur.execute(\\\"\\\"\\\"\\n        SELECT id, titulo, autor, isbn\\n        FROM livros\\n        WHERE status_cover = 0\\n        LIMIT ?\\n    \\\"\\\"\\\", (limit,))\\n\\n    rows = cur.fetchall()\\n    conn.close()\\n\\n    return rows\\n\\n\\n# =========================\\n# UPDATE\\n# =========================\\n\\ndef update_cover(book_id, url):\\n\\n    conn = get_conn()\\n    cur = conn.cursor()\\n\\n    cur.execute(\\\"\\\"\\\"\\n        UPDATE livros\\n        SET\\n            imagem_url = ?,\\n            status_cover = 1,\\n            updated_at = CURRENT_TIMESTAMP\\n        WHERE id = ?\\n    \\\"\\\"\\\", (url, book_id))\\n\\n    conn.commit()\\n    conn.close()\\n\\n\\n# =========================\\n# RUN\\n# =========================\\n\\ndef run(pacote=10):\\n\\n    rows = fetch_pending(pacote)\\n\\n    if not rows:\\n        log(\\\"Nada pendente para capas.\\\")\\n        return\\n\\n    processed = 0\\n    fallback_used = 0\\n    failed = 0\\n\\n    for book_id, titulo, autor, isbn in rows:\\n\\n        log(f\\\"CAPA → {titulo}\\\")\\n\\n        # 1️⃣ OpenLibrary\\n        cover = fetch_openlibrary_cover(isbn)\\n\\n        # 2️⃣ Google fallback\\n        if not cover:\\n\\n            cover = fetch_google_cover(\\n                titulo,\\n                autor\\n            )\\n\\n            if cover:\\n                fallback_used += 1\\n\\n        # 3️⃣ Falha\\n        if not cover:\\n\\n            failed += 1\\n            log(f\\\"SEM CAPA → {titulo}\\\")\\n            continue\\n\\n        update_cover(book_id, cover)\\n\\n        processed += 1\\n        log(f\\\"CAPA OK → {titulo}\\\")\\n\\n        time.sleep(0.2)\\n\\n    log(\\n        f\\\"CAPAS CONCLUÍDO → {processed} | fallback {fallback_used} | falhas {failed}\\\"\\n    )\\n\",\n    \"scripts\\\\steps\\\\dedup.py\": \"from difflib import SequenceMatcher\\nfrom datetime import datetime\\nimport os\\nimport sqlite3\\n\\n\\n# =========================\\n# DB PATH\\n# =========================\\n\\nDB_PATH = os.path.join(\\n    os.path.dirname(__file__),\\n    \\\"..\\\",\\n    \\\"data\\\",\\n    \\\"books.db\\\"\\n)\\n\\n\\ndef get_conn():\\n    return sqlite3.connect(DB_PATH, timeout=30)\\n\\n\\ndef log(msg):\\n    now = datetime.now().strftime(\\\"%H:%M:%S\\\")\\n    print(f\\\"[{now}] {msg}\\\")\\n\\n\\nSIMILARITY_THRESHOLD = 0.92\\n\\n\\n# =========================\\n# SIMILARIDADE\\n# =========================\\n\\ndef similar(a, b):\\n    return SequenceMatcher(None, a, b).ratio()\\n\\n\\n# =========================\\n# FETCH PENDENTES\\n# =========================\\n\\ndef fetch_pending(limit):\\n\\n    conn = get_conn()\\n    cur = conn.cursor()\\n\\n    cur.execute(\\\"\\\"\\\"\\n        SELECT id, titulo, slug, isbn\\n        FROM livros\\n        WHERE status_dedup = 0\\n        LIMIT ?\\n    \\\"\\\"\\\", (limit,))\\n\\n    rows = cur.fetchall()\\n    conn.close()\\n\\n    return rows\\n\\n\\n# =========================\\n# BUSCAR DUPLICADOS\\n# =========================\\n\\ndef find_duplicates(book):\\n\\n    conn = get_conn()\\n    cur = conn.cursor()\\n\\n    cur.execute(\\\"\\\"\\\"\\n        SELECT id, titulo, slug, isbn,\\n               descricao, imagem_url,\\n               ano_publicacao\\n        FROM livros\\n        WHERE id != ?\\n    \\\"\\\"\\\", (book[\\\"id\\\"],))\\n\\n    rows = cur.fetchall()\\n    conn.close()\\n\\n    duplicates = []\\n\\n    for r in rows:\\n\\n        if book[\\\"isbn\\\"] and r[3] == book[\\\"isbn\\\"]:\\n            duplicates.append(r)\\n            continue\\n\\n        if book[\\\"slug\\\"] and r[2] == book[\\\"slug\\\"]:\\n            duplicates.append(r)\\n            continue\\n\\n        if similar(book[\\\"titulo\\\"], r[1]) >= SIMILARITY_THRESHOLD:\\n            duplicates.append(r)\\n\\n    return duplicates\\n\\n\\n# =========================\\n# MERGE\\n# =========================\\n\\ndef merge_books(master_id, dup_row):\\n\\n    conn = get_conn()\\n    cur = conn.cursor()\\n\\n    dup_id = dup_row[0]\\n\\n    cur.execute(\\\"\\\"\\\"\\n        UPDATE livros\\n        SET\\n            descricao = COALESCE(descricao, ?),\\n            imagem_url = COALESCE(imagem_url, ?),\\n            ano_publicacao = COALESCE(ano_publicacao, ?),\\n            updated_at = ?\\n        WHERE id = ?\\n    \\\"\\\"\\\", (\\n        dup_row[4],\\n        dup_row[5],\\n        dup_row[6],\\n        datetime.utcnow(),\\n        master_id\\n    ))\\n\\n    cur.execute(\\n        \\\"DELETE FROM livros WHERE id = ?\\\",\\n        (dup_id,)\\n    )\\n\\n    conn.commit()\\n    conn.close()\\n\\n    log(f\\\"MERGE → {dup_id} → {master_id}\\\")\\n\\n\\n# =========================\\n# FLAG PROCESSADO\\n# =========================\\n\\ndef mark_processed(book_id):\\n\\n    conn = get_conn()\\n    cur = conn.cursor()\\n\\n    cur.execute(\\\"\\\"\\\"\\n        UPDATE livros\\n        SET status_dedup = 1,\\n            updated_at = ?\\n        WHERE id = ?\\n    \\\"\\\"\\\", (\\n        datetime.utcnow(),\\n        book_id\\n    ))\\n\\n    conn.commit()\\n    conn.close()\\n\\n\\n# =========================\\n# RUN\\n# =========================\\n\\ndef run(pacote=10):\\n\\n    rows = fetch_pending(pacot",
    "state\\backup\\2026-02-15_08-51_pipeline_full_part_002.json": "e)\\n\\n    if not rows:\\n        log(\\\"Nada pendente para dedup.\\\")\\n        return\\n\\n    processed = 0\\n    removed = 0\\n\\n    for r in rows:\\n\\n        book = {\\n            \\\"id\\\": r[0],\\n            \\\"titulo\\\": r[1],\\n            \\\"slug\\\": r[2],\\n            \\\"isbn\\\": r[3],\\n        }\\n\\n        duplicates = find_duplicates(book)\\n\\n        for dup in duplicates:\\n            merge_books(book[\\\"id\\\"], dup)\\n            removed += 1\\n\\n        mark_processed(book[\\\"id\\\"])\\n\\n        processed += 1\\n        log(f\\\"DEDUP OK → {book['titulo']}\\\")\\n\\n    log(\\n        f\\\"DEDUP CONCLUÍDO → processados {processed} | removidos {removed}\\\"\\n    )\\n\",\n    \"scripts\\\\steps\\\\export_state_transcript.py\": \"import os\\nimport json\\nimport sqlite3\\nfrom datetime import datetime\\nfrom pathlib import Path\\n\\n\\n# =========================\\n# ROOT\\n# =========================\\n\\nPROJECT_ROOT = Path(__file__).resolve().parents[2]\\nSTATE_DIR = PROJECT_ROOT / \\\"state\\\"\\n\\nSTATE_DIR.mkdir(exist_ok=True)\\n\\n\\n# =========================\\n# CONFIG\\n# =========================\\n\\nMAX_CHARS = 25000\\n\\nSITE_EXTENSIONS = [\\\".tsx\\\", \\\".ts\\\", \\\".css\\\", \\\".json\\\"]\\nPIPELINE_EXTENSIONS = [\\\".py\\\", \\\".json\\\", \\\".db\\\"]\\n\\nSITE_FOLDERS = [\\\"app\\\", \\\"lib\\\", \\\"public\\\"]\\nPIPELINE_FOLDERS = [\\\"scripts\\\", \\\"state\\\"]\\n\\nEXCLUDE_DIRS = {\\n    \\\"node_modules\\\",\\n    \\\".next\\\",\\n    \\\"venv\\\",\\n    \\\"__pycache__\\\"\\n}\\n\\nSQLITE_DB_PATH = PROJECT_ROOT / \\\"scripts\\\" / \\\"data\\\" / \\\"books.db\\\"\\n\\n\\n# =========================\\n# UTILS\\n# =========================\\n\\ndef now():\\n    return datetime.now().strftime(\\\"%Y-%m-%d_%H-%M\\\")\\n\\n\\n# =========================\\n# SPLIT\\n# =========================\\n\\ndef split_text(text):\\n\\n    parts = []\\n    start = 0\\n\\n    while start < len(text):\\n        parts.append(text[start:start + MAX_CHARS])\\n        start += MAX_CHARS\\n\\n    return parts\\n\\n\\ndef write_parts(name, data):\\n\\n    text = json.dumps(data, indent=2, ensure_ascii=False)\\n\\n    parts = split_text(text)\\n\\n    paths = []\\n\\n    for i, part in enumerate(parts, 1):\\n\\n        file = STATE_DIR / f\\\"{name}_part_{i:03}.json\\\"\\n\\n        with open(file, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n            f.write(part)\\n\\n        paths.append(file)\\n\\n    return paths\\n\\n\\n# =========================\\n# TREE (SITE REDUCED)\\n# =========================\\n\\ndef build_site_tree():\\n\\n    lines = []\\n\\n    for folder in SITE_FOLDERS:\\n\\n        base = PROJECT_ROOT / folder\\n\\n        if not base.exists():\\n            continue\\n\\n        for root, dirs, files in os.walk(base):\\n\\n            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]\\n\\n            rel = Path(root).relative_to(PROJECT_ROOT)\\n\\n            lines.append(str(rel))\\n\\n    return \\\"\\\\n\\\".join(lines)\\n\\n\\n# =========================\\n# FILE COLLECT\\n# =========================\\n\\ndef collect_files(folders, extensions=None):\\n\\n    collected = {}\\n\\n    for folder in folders:\\n\\n        base = PROJECT_ROOT / folder\\n\\n        if not base.exists():\\n            continue\\n\\n        for root, dirs, files in os.walk(base):\\n\\n            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]\\n\\n            for file in files:\\n\\n                if extensions and not any(\\n                    file.endswith(ext) for ext in extensions\\n                ):\\n                    continue\\n\\n                path = Path(root) / file\\n                rel = str(path.relative_to(PROJECT_ROOT))\\n\\n                if file.endswith(\\\".db\\\"):\\n                    collected[rel] = dump_sqlite(path)\\n                else:\\n                    collected[rel] = read_file(path)\\n\\n    return collected\\n\\n\\n# =========================\\n# FILE READ\\n# =========================\\n\\ndef read_file(path):\\n\\n    try:\\n        return path.read_text(encoding=\\\"utf-8\\\")\\n    except:\\n        return \\\"[binary or unreadable]\\\"\\n\\n\\n# =========================\\n# SQLITE DUMP\\n# =========================\\n\\ndef dump_sqlite(path):\\n\\n    try:\\n\\n        conn = sqlite3.connect(path)\\n        cursor = conn.cursor()\\n\\n        cursor.execute(\\n            \\\"SELECT name FROM sqlite_master WHERE type='table';\\\"\\n        )\\n\\n        tables = [t[0] for t in cursor.fetchall()]\\n\\n        dump = {}\\n\\n        for table in tables:\\n\\n            cursor.execute(f\\\"PRAGMA table_info({table});\\\")\\n            cols = cursor.fetchall()\\n\\n            dump[table] = [\\n                {\\n                    \\\"name\\\": c[1],\\n                    \\\"type\\\": c[2]\\n                }\\n                for c in cols\\n            ]\\n\\n        conn.close()\\n\\n        return dump\\n\\n    except:\\n        return {\\\"error\\\": \\\"failed to read sqlite\\\"}\\n\\n\\n# =========================\\n# DB ABSTRACT\\n# =========================\\n\\ndef load_project_state():\\n\\n    path = PROJECT_ROOT / \\\"project_state.json\\\"\\n\\n    if path.exists():\\n        return path.read_text(encoding=\\\"utf-8\\\")\\n\\n    return \\\"project_state.json não encontrado.\\\"\\n\\n\\ndef load_db_schema():\\n\\n    path = PROJECT_ROOT / \\\"database_schema.json\\\"\\n\\n    if path.exists():\\n        return path.read_text(encoding=\\\"utf-8\\\")\\n\\n    return \\\"database_schema.json não encontrado.\\\"\\n\\n\\n# =========================\\n# MODES\\n# =========================\\n\\ndef export_site():\\n\\n    name = f\\\"{now()}_site_bootstrap\\\"\\n\\n    data = {\\n\\n        \\\"tree_site\\\": build_site_tree(),\\n\\n        \\\"project_state\\\": load_project_state(),\\n\\n        \\\"database_schema_abstract\\\": load_db_schema(),\\n\\n        \\\"files\\\": collect_files(\\n            SITE_FOLDERS,\\n            SITE_EXTENSIONS\\n        )\\n    }\\n\\n    return write_parts(name, data)\\n\\n\\ndef export_pipeline():\\n\\n    name = f\\\"{now()}_pipeline_full\\\"\\n\\n    data = {\\n\\n        \\\"sqlite_dump\\\": dump_sqlite(SQLITE_DB_PATH),\\n\\n        \\\"files\\\": collect_files(\\n            PIPELINE_FOLDERS,\\n            PIPELINE_EXTENSIONS\\n        )\\n    }\\n\\n    return write_parts(name, data)\\n\\n\\ndef export_full():\\n\\n    name = f\\\"{now()}_full_snapshot\\\"\\n\\n    data = {\\n\\n        \\\"tree_site\\\": build_site_tree(),\\n\\n        \\\"sqlite_dump\\\": dump_sqlite(SQLITE_DB_PATH),\\n\\n        \\\"files\\\": collect_files(\\n            [\\\".\\\"],\\n            None\\n        )\\n    }\\n\\n    return write_parts(name, data)\\n\\n\\n# =========================\\n# ENTRY\\n# =========================\\n\\ndef export_state_transcript(mode=\\\"site\\\"):\\n\\n    if mode == \\\"site\\\":\\n        paths = export_site()\\n\\n    elif mode == \\\"pipeline\\\":\\n        paths = export_pipeline()\\n\\n    elif mode == \\\"full\\\":\\n        paths = export_full()\\n\\n    else:\\n        print(\\\"Modo inválido.\\\")\\n        return\\n\\n    print(\\\"\\\\nTranscript gerado:\\\\n\\\")\\n\\n    for p in paths:\\n        print(p)\\n\\n    print(\\\"\\\\nTotal partes:\\\", len(paths), \\\"\\\\n\\\")\\n\\n\\n# =========================\\n\\nif __name__ == \\\"__main__\\\":\\n    export_state_transcript(\\\"site\\\")\\n\",\n    \"scripts\\\\steps\\\\prospect.py\": \"# ============================================\\n# LIVRARIA ALEXANDRIA — PROSPECT\\n# Compatível com books.db (schema staging)\\n# ID Strategy: UUID v4 + fallback hash título\\n# ============================================\\n\\nimport os\\nimport time\\nimport uuid\\nimport hashlib\\nimport sqlite3\\nimport requests\\n\\nfrom datetime import datetime\\nfrom steps._clusters import CLUSTERS\\n\\n\\n# ============================================\\n# CONFIG\\n# ============================================\\n\\nDB_PATH = os.path.join(\\n    os.path.dirname(__file__),\\n    \\\"..\\\",\\n    \\\"data\\\",\\n    \\\"books.db\\\"\\n)\\n\\nOPENLIBRARY_URL = \\\"https://openlibrary.org/search.json\\\"\\nGOOGLE_BOOKS_URL = \\\"https://www.googleapis.com/books/v1/volumes\\\"\\n\\nHEARTBEAT_INTERVAL = 30\\n\\n\\n# ============================================\\n# HEARTBEAT\\n# ============================================\\n\\n_last_event = time.time()\\n\\n\\ndef ts():\\n    return datetime.now().strftime(\\\"%H:%M:%S\\\")\\n\\n\\ndef beat(msg=\\\"Script ativo…\\\"):\\n    global _last_event\\n\\n    now_ts = time.time()\\n\\n    if now_ts - _last_event >= HEARTBEAT_INTERVAL:\\n        print(\\n            f\\\"[{ts()}] {msg} último evento há {int(now_ts - _last_event)}s\\\"\\n        )\\n\\n    _last_event = now_ts\\n\\n\\n# ============================================\\n# DB\\n# ============================================\\n\\ndef get_conn():\\n    return sqlite3.connect(DB_PATH)\\n\\n\\ndef ensure_schema():\\n\\n    conn = get_conn()\\n    cur = conn.cursor()\\n\\n    cur.execute(\\\"\\\"\\\"\\n    CREATE TABLE IF NOT EXISTS livros (\\n        id TEXT PRIMARY KEY,\\n        titulo TEXT NOT NULL,\\n        slug TEXT UNIQUE,\\n        autor TEXT,\\n        descricao TEXT,\\n        isbn TEXT,\\n        ano_publicacao INTEGER,\\n        imagem_url TEXT,\\n        idioma TEXT,\\n        cluster TEXT,\\n        fonte TEXT,\\n        status_slug INTEGER DEFAULT 0,\\n        status_dedup INTEGER DEFAULT 0,\\n        status_synopsis INTEGER DEFAULT 0,\\n        status_review INTEGER DEFAULT 0,\\n        status_cover INTEGER DEFAULT 0,\\n        status_publish INTEGER DEFAULT 0,\\n        created_at DATETIME,\\n        updated_at DATETIME\\n    )\\n    \\\"\\\"\\\")\\n\\n    conn.commit()\\n    conn.close()\\n\\n\\n# ============================================\\n# ID STRATEGY\\n# ============================================\\n\\ndef generate_id(titulo, isbn):\\n\\n    if isbn:\\n        base = isbn\\n    else:\\n        base = titulo\\n\\n    hash_id = hashlib.sha1(\\n        base.encode(\\\"utf-8\\\")\\n    ).hexdigest()\\n\\n    return str(uuid.uuid4())[:8] + hash_id[:16]\\n\\n\\n# ============================================\\n# INSERT\\n# ============================================\\n\\ndef exists(titulo):\\n\\n    conn = get_conn()\\n    cur = conn.cursor()\\n\\n    cur.execute(\\n        \\\"SELECT 1 FROM livros WHERE titulo = ? LIMIT 1\\\",\\n        (titulo,)\\n    )\\n\\n    res = cur.fetchone()\\n    conn.close()\\n\\n    return res is not None\\n\\n\\ndef insert_book(data, cluster):\\n\\n    if exists(data[\\\"titulo\\\"]):\\n        print(f\\\"[{ts()}] SKIP duplicado → {data['titulo']}\\\")\\n        return False\\n\\n    livro_id = generate_id(\\n        data[\\\"titulo\\\"],\\n        data[\\\"isbn\\\"]\\n    )\\n\\n    now = datetime.utcnow()\\n\\n    conn = get_conn()\\n    cur = conn.cursor()\\n\\n    cur.execute(\\\"\\\"\\\"\\n        INSERT INTO livros (\\n            id,\\n            titulo,\\n            autor,\\n            isbn,\\n            ano_publicacao,\\n            idioma,\\n            cluster,\\n            fonte,\\n            created_at,\\n            updated_at\\n        )\\n        VALUES (?, ?, ?